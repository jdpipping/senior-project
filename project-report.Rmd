---
title: "Implementing LASSO Regression to Examine County-Level Determinants of Mental Health"
author: "Jonathan Pipping"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: pdflatex
bibliography: project-report.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load libraries, data, seed, message = F, warning = FALSE}
# load libraries
library(broom)
library(car)
library(gridExtra)
library(glmnet)
library(kableExtra)
library(knitr)
library(selectiveInference)
library(tidyverse)

# read in data
county_data = read_csv('final-data.csv')

# set seed
set.seed(2023)
```

# Introduction

Few would argue that mental health represents an undeniable concern in the United States. The U.S. Substance Abuse and Mental Health Services Administration (SAMHSA) estimates that over one in five adults (59.3 million) live with a mental illness. Furthermore, an estimated 45.3% of these individuals receive treatment, leaving approximately 29 million Americans without care [@samhsa]. In response to these staggering numbers, the U.S. Congress passed the Bipartisan Safer Communities Act (BSCA) in 2022, which included \$250 million in supplemental funding to help states and territories address the mental health challenges facing their communities [@usdhhs].

In the context of this recent legislation, this research aims to establish a framework for accurate mental health prediction on a county level, identify significant social, demographic, and behavioral determinants of mental health, and provide insights to state and local officials to aid their efforts in limiting the prevalence of poor mental health days within their communities.

# Data

The data for this project was obtained from the University of Wisconsin's 2023 County Health Rankings data set, which is publicly available on the Harvard Dataverse [@usclhr]. Consisting of publicly-recorded measures and self-reported survey results, this data set includes over 70 county-level metrics such as life expectancy, median household income, and the percentage of people uninsured. All metrics are accompanied by 95% confidence intervals and associated ranks, but this project's scope relies only on point estimates.

The data was subsequently cleaned and prepared for analysis. This process involved the omission of metrics flagged as "highly unreliable" by the University of Wisconsin's research team. Additionally, metrics with high (>10%) missingness were omitted to preserve a high number of complete cases for regression. After cleaning, the final data set comprised 61 metrics from 2,636 US counties, including the average percentage of days people in each county reported feeling mentally unhealthy (the outcome of interest).

# Methods and Results

##  Linear Regression

```{r ols, message = F, warning = F}

# model fit
ols_fit = county_data |> 
  dplyr::select(-fips, -state, -county) |> 
  mutate_all(scale) |> 
  lm(formula = perc_days_mentally_unhealthy ~ .)

# extract mse
ols_mse = summary(ols_fit)$sigma^2
# extract adjusted r squared
adj_r_squared = summary(ols_fit)$adj.r.squared

# regression table
ols_table = tidy(ols_fit) |> 
  arrange(desc(estimate)) |> 
  mutate(sig = case_when(p.value < 0.001 ~ 'Yes', T ~ 'No')) |> 
  rename(Term = term,
         Estimate = estimate,
         `Standard Error` = std.error,
         `Test Statistic` = statistic,
         `P-Value` = p.value,
         `Significant?*` = sig)

# significant predictors
ols_significant = ols_table |> 
  filter(`Significant?*` == 'Yes') |> 
  dplyr::select(-`Significant?*`)

# ols residuals vs fitted values

# split covariates, outcome
covariates = county_data |> 
  dplyr::select(-fips, - state, -county, -perc_days_mentally_unhealthy) |> 
  mutate_all(scale) |> 
  as.matrix()
outcome = county_data |> 
  dplyr::select(perc_days_mentally_unhealthy) |> 
  mutate_all(scale) |> 
  as.matrix()
# ols fitted values
ols_fitted = predict(ols_fit, newx = covariates)
# ols residuals
ols_residuals = outcome - ols_fitted
# plot
ols_residual_plot = ols_fitted |> 
  bind_cols(ols_residuals) |> 
  ggplot(aes(x = ols_fitted, y = ols_residuals)) +
  labs(x = 'Fitted Vallues', y = 'Residuals') +
  geom_point(pch = 20) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# vif table

# calculate vif
vif = vif(ols_fit)
# compile results in table
vif_table = tibble(variables = names(vif), vif = vif) |> 
  arrange(desc(vif))

```

After standardizing all variables, a multiple linear regression was fit to isolate potential causal relationships between the covariates and our outcome of interest. This model can be represented in the form $\hat Y_i = \beta_0 + \sum_{j=1}^{60} X_{ij} \beta_{j}+ \epsilon_i, i\in\{1,2,\ldots,2,636\}$ where $\hat Y_i$ represents the estimated outcome (the average percentage of days people in a particular county reported feeling mentally unhealthy), the $X_{ij}$ represent the 2,636 observed values of each of our 60 covariates, the $\beta_j$ represent the coefficients that satisfy the Least-Squares solution $\boldsymbol{\hat{\beta}_{\text{OLS}}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$, and the $e_i$ represent the error (or residual) for each $\hat Y_i$ prediction. The OLS estimates for each $\beta_j$ coefficient are shown in **Table 1** below.

```{r ols table, message = F, warning = F}
kable(ols_table, format = 'latex', booktabs = T, caption = "Linear Regression Results") |> 
  kable_styling(font_size = 7) |> 
   footnote(general = '* At 0.001 Significance Level')
```

\newpage

```{r ols significant, message = F, warning = F}
kable(ols_significant, format = 'latex', booktabs = T, caption = "OLS, Statistically Significant Predictors") |> 
  kable_styling(font_size = 7) |> 
   footnote(general = '* At 0.001 Significance Level')
```

Based on these results, there are 23 statistically significant predictors of the average percentage of days people reported feeling mentally unhealthy (at the $\alpha = 0.001$ confidence level). These predictors and coefficient estimates are shown in **Table 2** above. Of these covariates, the strongest positive predictor of poor mental health days is the average percentage of days people from that county reported feeling physically unhealthy. All other things equal, every one-standard-deviation increase in this variable results in an estimated increase in a county's average percentage of poor mental health days of 0.9691171 standard deviations. All other estimated coefficients can be interpreted similarly.

In addition to a relatively low MSE of `r round(ols_mse, 4)` and a high adjusted R-squared of `r round(adj_r_squared, 4)`, preliminary residual analysis indicates a good model fit. A plot of model residuals against fitted values is shown in **Figure 1** below. Most residuals are between -0.5 and 0.5, and none have an absolute value greater than 2. Considering that our sample includes 2,636 observations, this is an encouraging result that validates our model choice.

```{r ols residuals vs fitted values, fig.height = 2, fig.width = 4, fig.cap = 'OLS Residuals vs Fitted Values', message = F, warning = F}
ols_residual_plot
```

\newpage

```{r vif}
kable(head(vif_table, 20), format = 'latex', booktabs = T, caption = "Variance Inflation Factor, Top 20 Covariates") |> 
  kable_styling(font_size = 9)
```

However, including all 60 covariates in this regression presents some issues, especially regarding multicollinearity and the subsequent interpretability of our estimated $\beta_j$s. The Variance Inflation Factor (VIF) measures the extent to which predictors in a multiple regression model are correlated. Values between 5 and 10 generally cause mild concern, and values over 10 are potentially problematic. As **Table 3** above demonstrates, 18 of our 60 covariates have a VIF that exceeds this threshold.

Upon further inspection, it is clear that the variables with high variance inflation factors would be highly correlated with each other and, in some cases, even deterministic (ethnicity proportions summing to 1). Though this result does not compromise the model's predictive ability, accurate interpretation of its coefficients becomes nearly impossible. Therefore, we must implement a variable selection procedure to weed out surplus and correlated predictors while maintaining model integrity. With this in mind, we turn our focus to LASSO Regression.

## LASSO Regression

```{r lasso, message = F, warning = F}

# model fit w/ 10-fold cv
lasso_fit = cv.glmnet(covariates, outcome, alpha = 1)
# extract coefficients
lasso_coef = tidy(lasso_fit$glmnet.fit)

# optimal lambda, 1se criteria
cv_lambda = lasso_fit$lambda.1se
# optimal coefficients
cv_coef = tidy(lasso_fit$glmnet.fit) |> 
  filter(lambda == cv_lambda) |> 
  dplyr::select(term, estimate) |> 
  mutate(
    # coefficient standard error
    se = c(NA, fixedLassoInf(x = covariates, y = outcome, beta = as.vector(coef(lasso_fit, x = covariates, y = outcome, s = cv_lambda))[-1], lambda = cv_lambda, alpha = 0.001)$sd),
    # test statistic
    test_statistic = estimate / se,
    # p value
    p_value = pnorm(abs(test_statistic), lower.tail = F),
    )

# extract mse
lasso_mse = lasso_fit$cvm[which(lasso_fit$lambda == cv_lambda)]

# regression table
lambda_table = cv_coef |> 
  filter(term != '(Intercept)') |> 
  arrange(desc(estimate)) |> 
  mutate(sig = case_when(p_value < 0.001 ~ 'Yes', T ~ 'No')) |> 
  rename(Term = term,
         Estimate = estimate,
         `Standard Error` = se,
         `Test Statistic` = test_statistic,
         `P-Value` = p_value,
         `Significant?*` = sig)

# significant predictors
lasso_significant = lambda_table |> 
  filter(`Significant?*` == 'Yes') |> 
  dplyr::select(-`Significant?*`)


# coefficient estimates vs lambda
coef_vs_lambda = lasso_coef |>
  ggplot(aes(x = lambda, y = estimate, group = term)) +
  labs(x = expression(lambda), y = 'Coefficient Estimate', title = expression(paste('Coefficient Estimates vs ', lambda))) +
  scale_x_log10() +
  geom_line(alpha = 0.75) +
  geom_vline(xintercept = lasso_fit$lambda.min) +
  geom_vline(xintercept = cv_lambda, 
             linetype = 'dashed', color = 'red') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# number of non-zero predictors vs lambda
nonzero_vs_lambda = lasso_fit |>
  tidy() |> 
  ggplot(aes(x = lambda, y = nzero)) +
  labs(x = expression(lambda), y = 'Non-Zero Predictors', title = expression(paste('Number of Non-Zero Predictors vs ', lambda))) +
  geom_line() +
  geom_vline(xintercept = cv_lambda, 
             linetype = 'dashed', color = 'red') +
  scale_x_log10() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# residuals vs fitted values

# lasso fitted
lasso_fitted = predict(lasso_fit, newx = covariates, s = cv_lambda)
# lasso residuals
lasso_residuals = outcome - lasso_fitted
# plot
lasso_residual_plot = lasso_fitted |> 
  bind_cols(lasso_residuals) |> 
  ggplot(aes(x = lasso_fitted, y = lasso_residuals)) +
  labs(x = 'Fitted Vallues', y = 'Residuals', title = 'Residuals vs Fitted Values') +
  geom_point(pch = 20) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# LASSO coefficient plot
lasso_coef_plot = cv_coef |> 
  mutate(coef_sign = as.factor(sign(estimate)),
         term = fct_reorder(term, estimate)) %>%
  ggplot(aes(x = term, y = estimate, fill = coef_sign)) +
  labs(x = 'Term', y = 'LASSO Estimate') +
  geom_bar(stat = 'identity', color = 'white') +
  scale_fill_manual(values = c('darkred', 'darkblue'), 
                    guide = FALSE) +
  coord_flip() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

LASSO, the "Least Absolute Shrinkage and Selection Operator," is a statistical method used for regularization and variable selection for linear models. Instead of calculating $\beta_j$s using the ordinary least squares solution $\hat{\beta}_{\text{OLS}} = \underset{\beta}{\text{argmin}} \left\{ \frac{1}{2N} \sum_{i=1}^{N} (Y_i - \beta_0 - \sum_{j=1}^{p} X_{ij}\beta_j)^2 \right\}$, LASSO introduces an additional penalty term $\lambda \sum_{j=1}^{p} |\beta_j|$ to the optimization problem. This term is proportional to the sum of the absolute value of $\beta_j$'s and induces both bias and sparsity as it pushes all $\beta_j$s closer to 0. The strength of this penalty is governed by $\lambda$, which is typically tuned with k-fold cross-validation [@psu].

In the context of our data set, the LASSO serves as a useful methodology for variable selection (eliminating surplus predictors with OLS coefficients close to 0 and limiting multicollinearity) while preserving the model's predictive ability and linear structure. After standardizing all variables, a LASSO regression model was fit on the data set, and $\beta_j$s were calculated according to the LASSO solution $\hat{\beta}_{\text{lasso}} = \underset{\beta}{\text{argmin}} \left\{ \frac{1}{2 \times 2,636} \sum_{i=1}^{2,636} (Y_i - \beta_0 - \sum_{j=1}^{60} x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^{60} |\beta_j| \right\}$, with $\lambda$ tuned by 10-fold cross-validation. The optimal LASSO estimates (at $\lambda =$ `r round(cv_lambda, 4)`) for each non-zero $\beta_j$ coefficient are shown in **Table 4** below.

```{r lasso table, message = F, warning = F}
kable(lambda_table, format = 'latex', booktabs = T, caption = "LASSO Regression Results") |> 
  kable_styling(font_size = 8) |> 
   footnote(general = '* At 0.001 Significance Level')
```

\newpage

```{r lasso significant, message = F, warning = F}
kable(lasso_significant, format = 'latex', booktabs = T, caption = "LASSO, Statistically Significant Predictors") |> 
  kable_styling(font_size = 7) |> 
   footnote(general = '* At 0.001 Significance Level')
```

The results in **Table 4** only include 45 coefficients, indicating that the LASSO shrunk the coefficients of 15 weakly-relevant covariates to 0. Additionally, only 16 variables were deemed statistically significant predictors of our outcome (at $\alpha = 0.001$), and each of their coefficients is slightly closer to 0 than their corresponding OLS estimate. Unsurprisingly, the average percentage of days spent physically unhealthy remains the clearest predictor of the average percentage of days spent mentally unhealthy. Meanwhile, no ethnicity percentages were deemed statistically significant (OLS had 5). These results, displayed in **Table 5** above, showcase the power of LASSO for shrinkage and variable selection, even in data sets riddled with multicollinearity. Additionally, **Figure 2** provides a visualization of how the strength of these properties varies across different values of $\lambda$.

```{r effects of lasso, fig.height = 4, fig.width = 8, fig.cap = 'LASSO Visualizations', message = F, warning = F}
grid.arrange(coef_vs_lambda, nonzero_vs_lambda, ncol = 2)
```

\newpage

Despite eliminating 15 variables through shrinkage, the LASSO only increased the MSE by `r round(lasso_mse - ols_mse, 4)` compared to our linear regression model. Additionally, this algorithm does not induce outliers, as evidenced by **Figure 3** below. This unique ability to preserve the predictive ability of a linear regression while making its covariates more independent and its coefficients more interpretable sets the LASSO apart as a viable statistical method for econometric analysis.

```{r lasso residuals vs fitted values, fig.height = 4, fig.width = 6, fig.cap = 'LASSO Residuals vs Fitted Values', message = F, warning = F}
ols_residual_plot
```

Turning our attention to the problem at hand, it is clear that physical health is highly connected to mental health: all else held equal, a one-standard-deviation increase in a county's average percentage of days reported as physically unhealthy results in a 0.9174263 SD increase in its average percentage of days reported as mentally unhealthy. This finding supports the biopsychosocial model of health, which emphasizes the connectivity of biological, social, and psychological wellness [@wustl]. The two next-most significant positive predictors (the percentage of adults who report less than 7 hours of sleep per night and the percentage of adults who suffer food insecurity) also directly support this framework, together indicating that addressing mental health concerns on a county level necessitates investment in both mental health treatment and support for impoverished and overworked communities that enable them to live healthy lives.

However, the significant negative coefficient associated with the percentage of physically inactive adults is puzzling, as it starkly contrasts the marginal association between these variables. This indicates either the presence of a confounding variable or of continued multicollinearity in the LASSO regression setting. Additionally, the significant positive coefficients associated with median household income and broadband internet access indicate that this study may suffer from biased sampling or self-reporting bias.

Finally, the significance of demographic variables, such as the percentage of females in a county and the percentage of (non) working-age people (18-65) in predicting poor mental health, may indicate under-reporting in specific demographics. However, it could also guide government officials to focus their resources and efforts on helping these groups of people.

```{r lasso coefficient plot, fig.height = 8, fig.width = 6, fig.cap = 'LASSO Coefficient Plot', message = F, warning = F}
lasso_coef_plot
```

\newpage

# Conclusions

Though least-squares regression remains an essential econometric method, the interpretation of its coefficients is easily complicated by multicollinearity in higher-dimensional settings. We argue the viability of LASSO regression as a solution to this problem based on its properties, which preserve OLS' inferential and predictive properties while reducing multicollinearity and increasing the interpretability of its coefficients. Finally, we interpret the optimal LASSO coefficients and provide policy recommendations to help state and local government officials address their communities' mental health challenges. Future work includes investigating methods to de-bias LASSO coefficients for inference when the optimal $\lambda$ is large and developing variance inflation factor methodology to quantify the effect of LASSO regression on multicollinearity more accurately.

# References
